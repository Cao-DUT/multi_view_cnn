{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torchvision\n",
    "import torch.optim.lr_scheduler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "EPOCH = 50\n",
    "GAMMA = 0.9\n",
    "STEP_SIZE = 200\n",
    "LR = 0.0001\n",
    "USE_GPU = True\n",
    "decoder = ['buoy', 'dock', 'light_buoy', 'totem']\n",
    "data_transform = transforms.Compose([\n",
    "            transforms.Resize(227),\n",
    "            #transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "img_path = '/home/arg_ws3/david_trainings/image_data'\n",
    "model_path = './model_scale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=4096, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "alexnet = torchvision.models.alexnet(pretrained = True)\n",
    "alexnet.classifier[6] = nn.Linear(4096, 4)\n",
    "if USE_GPU:\n",
    "    alexnet = alexnet.cuda()\n",
    "#optimizer = torch.optim.Adam(alexnet.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "optimizer = torch.optim.SGD(alexnet.parameters(), lr = LR, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss() # the target label is not one-hotted\n",
    "if USE_GPU:\n",
    "    loss_func = loss_func.cuda()\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiViewNet(\n",
      "  (base_features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (base_classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "  )\n",
      "  (new_classifier): Sequential(\n",
      "    (0): Linear(in_features=4099, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MultiViewNet(nn.Module):\n",
    "    def __init__(self, base_model, num_classes):\n",
    "        super(MultiViewNet, self).__init__()\n",
    "        # Everything except the last linear layer\n",
    "        self.base_features = base_model.features\n",
    "        self.base_classifier = nn.Sequential(*base_model.classifier[:-1])\n",
    "        self.new_classifier = nn.Sequential(\n",
    "            nn.Linear(4099, num_classes)\n",
    "        )\n",
    "        self.modelName = 'MultiViewNet'\n",
    "        \n",
    "        '''\n",
    "        # Freeze those weights\n",
    "        for p in self.features.parameters():\n",
    "            p.requires_grad = False\n",
    "        '''\n",
    "\n",
    "    def forward(self, img, scale):\n",
    "        f = self.base_features(img)\n",
    "        f = f.view(f.size(0), -1)\n",
    "        fc2 = self.base_classifier(f)\n",
    "        fc2_cat = torch.cat((scale[0], scale[1], scale[2], fc2), dim=1)\n",
    "        y = self.new_classifier(fc2_cat)\n",
    "        return y\n",
    "\n",
    "base_model = torchvision.models.alexnet(pretrained = True)\n",
    "multiviewnet = MultiViewNet(base_model, 4)\n",
    "if USE_GPU:\n",
    "    multiviewnet = multiviewnet.cuda()\n",
    "#for p in multiviewnet.base_features[0].parameters():\n",
    "#    print(p.name, p.data)\n",
    "#optimizer = torch.optim.Adam(alexnet.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "optimizer = torch.optim.SGD(multiviewnet.parameters(), lr = LR, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss() # the target label is not one-hotted\n",
    "if USE_GPU:\n",
    "    loss_func = loss_func.cuda()\n",
    "print(multiviewnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiViewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root = '../images', transform = None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.imgData = datasets.ImageFolder(root = self.root, \\\n",
    "                                transform = self.transform)\n",
    "        self.classes = self.imgData.classes\n",
    "        self.class_to_idx = self.imgData.class_to_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.imgData.__len__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgData.__getitem__(idx)[0]\n",
    "        label = self.imgData.__getitem__(idx)[1]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiViewScaleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root = '../images', transform = None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.imgData = datasets.ImageFolder(root = self.root, \\\n",
    "                                transform = self.transform)\n",
    "        self.classes = self.imgData.classes\n",
    "        self.class_to_idx = self.imgData.class_to_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.imgData.__len__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgData.__getitem__(idx)[0]\n",
    "        label = self.imgData.__getitem__(idx)[1]\n",
    "        img_name = self.imgData.samples[idx][0]\n",
    "        s = img_name.split('.jpg')\n",
    "        s = s[0].split('/')\n",
    "        scalar_nums = s[-1].split('_')[-3:]\n",
    "        scalar_nums = [torch.FloatTensor([math.log(float(i))*2]) for i in scalar_nums]\n",
    "        if len(scalar_nums)!=3:\n",
    "            return img, label, ['0', '0', '0']\n",
    "        return img, label, scalar_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "'''class MultiViewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root = '../images', transform = None, extensions = ['.jpg']):\n",
    "        classes, class_to_idx = self.find_classes(root)\n",
    "        samples = make_dataset(root, class_to_idx, IMG_EXTENSIONS)\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.samples = samples\n",
    "\n",
    "    def has_file_allowed_extension(self, filename, extensions):\n",
    "        \"\"\"Checks if a file is an allowed extension.\n",
    "        Args:\n",
    "            filename (string): path to a file\n",
    "            extensions (iterable of strings): extensions to consider (lowercase)\n",
    "        Returns:\n",
    "            bool: True if the filename ends with one of given extensions\n",
    "        \"\"\"\n",
    "        filename_lower = filename.lower()\n",
    "        return any(filename_lower.endswith(ext) for ext in extensions)\n",
    "    \n",
    "    def make_dataset(self, dir, class_to_idx, extensions):\n",
    "        images = []\n",
    "        dir = os.path.expanduser(dir)\n",
    "        for target in sorted(class_to_idx.keys()):\n",
    "            d = os.path.join(dir, target)\n",
    "            if not os.path.isdir(d):\n",
    "                continue\n",
    "\n",
    "            for root, _, fnames in sorted(os.walk(d)):\n",
    "                for fname in sorted(fnames):\n",
    "                    if self.has_file_allowed_extension(fname, extensions):\n",
    "                        path = os.path.join(root, fname)\n",
    "                        item = (path, class_to_idx[target])\n",
    "                        images.append(item)\n",
    "        return images\n",
    "        \n",
    "    def find_classes(self, dir):\n",
    "        if sys.version_info >= (3, 5):\n",
    "            # Faster and available in Python 3.5 and above\n",
    "            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
    "        else:\n",
    "            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "        classes.sort()\n",
    "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        return classes, class_to_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, target = self.samples[idx]\n",
    "        return path, target'''\n",
    "print('skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use torch ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_loader = torch.utils.data.DataLoader(dataset = train_data,                                 batch_size = BATCH_SIZE,                                 shuffle = True)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_data = datasets.ImageFolder(root = img_path, \\\n",
    "                                transform = data_transform)'''\n",
    "\n",
    "'''train_loader = torch.utils.data.DataLoader(dataset = train_data, \\\n",
    "                                batch_size = BATCH_SIZE, \\\n",
    "                                shuffle = True)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buoy', 'dock', 'light_buoy', 'totem']\n"
     ]
    }
   ],
   "source": [
    "MultiViewDst = MultiViewScaleDataset(root = img_path,\\\n",
    "                            transform = data_transform)\n",
    "decoder = MultiViewDst.classes\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset to training & validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(MultiViewDst) #3080 #2080\n",
    "indices = list(range(dataset_size))\n",
    "validation_split = .1\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = MultiViewDst, \\\n",
    "                                batch_size = BATCH_SIZE, \\\n",
    "                                sampler = train_sampler)\n",
    "validation_size = len(val_indices)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset = MultiViewDst, \\\n",
    "                                batch_size = validation_size, \\\n",
    "                                sampler = valid_sampler)\n",
    "valid_iter = iter(validation_loader)\n",
    "valid_data = next(valid_iter)\n",
    "valid_y = valid_data[1].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_scale(model, validset, label):\n",
    "    if USE_GPU:\n",
    "        validset[0] = validset[0].cuda()\n",
    "        scale = [x.cuda() for x in validset[2]]\n",
    "    output = model(validset[0], scale)\n",
    "    pred_y = torch.max(output, 1)[1].data.squeeze()\n",
    "    accuracy = (pred_y == label).sum().item() / float(label.size(0))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, validset, label):\n",
    "    if USE_GPU:\n",
    "        validset[0] = validset[0].cuda()\n",
    "    output = model(validset[0])\n",
    "    pred_y = torch.max(output, 1)[1].data.squeeze()\n",
    "    accuracy = (pred_y == label).sum().item() / float(label.size(0))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 42.860512 | accuracy: 0.2816 | learning rate: 0.000100\n",
      "Epoch:  1 | train loss: 0.179864 | accuracy: 0.9563 | learning rate: 0.000100\n",
      "Epoch:  2 | train loss: 0.072124 | accuracy: 0.9806 | learning rate: 0.000100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-37f300d0ec9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_scale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# gives batch data, normalize x when iterate train_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mUSE_GPU\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mb_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-fab0647dbb52>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \"\"\"\n\u001b[1;32m    100\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \"\"\"\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "count = 0\n",
    "loss_values = []\n",
    "lr = LR\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y, b_scale) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "        if USE_GPU:\n",
    "            b_x = b_x.cuda()\n",
    "            b_y = b_y.cuda()\n",
    "            b_scale = [x.cuda() for x in b_scale]\n",
    "        #print(b_x.shape)\n",
    "        output = multiviewnet(b_x, b_scale)  # cnn output\n",
    "        loss = loss_func(output, b_y)   # cross entropy loss\n",
    "        #loss_values.append(loss.item())\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients\n",
    "        \n",
    "        #scheduler.step()                # dynamic learning rate\n",
    "        \n",
    "        if count % STEP_SIZE == 0 and count != 0:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= GAMMA\n",
    "                lr = param_group['lr']\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            accuracy = compute_accuracy_scale(multiviewnet, valid_data, valid_y)\n",
    "            print('Epoch: ', epoch, '| train loss: %.6f' % loss.cpu().data.numpy(), \\\n",
    "                  '| accuracy: %.4f' % accuracy, \\\n",
    "                  '| learning rate: %.6f' % lr)\n",
    "        if count % 10 == 0 and count != 0:\n",
    "            loss_values.append(loss.item())\n",
    "        if count % 200 == 0 and count != 0:\n",
    "            plt.plot(loss_values, '-b', label='loss')\n",
    "            plt.show()\n",
    "        count = count + 1\n",
    "    if epoch % 5 == 0 and epoch != 0:\n",
    "        PATH = model_path + '/robotx_ch3_epoch' + str(epoch) + '.pth'\n",
    "        torch.save(multiviewnet.state_dict(), PATH)\n",
    "        print(\"Save net: \", PATH)\n",
    "print(\"Finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Dropout(p=0.5)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Linear(in_features=4096, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet.load_state_dict(torch.load('../model/robotx_ch3_epoch25.pth'))\n",
    "alexnet.eval()\n",
    "#alexnet = torch.load('./robotx_ch3.pth')\n",
    "#alexnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loader = torch.utils.data.DataLoader(dataset = MultiViewDst, \\\n",
    "                                batch_size = 1, \\\n",
    "                                sampler = valid_sampler)\n",
    "show_iter = iter(show_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 227, 227])\n",
      "Prediction:  buoy\n",
      "Label:  buoy\n",
      "Image shape:  (227, 227, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f405cb55cf8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = next(show_iter)\n",
    "img = data[0][0].numpy()\n",
    "torch_img = np.expand_dims(img, axis=0)\n",
    "x = torch.tensor(torch_img).type('torch.FloatTensor').cuda()\n",
    "output = alexnet(x)\n",
    "print(x.shape)\n",
    "pred_y = torch.max(output, 1)[1].cpu().data.numpy()\n",
    "print('Prediction: ', decoder[int(pred_y)])\n",
    "\n",
    "print('Label: ', decoder[data[1][0].numpy()])\n",
    "#img = img[:][:]\n",
    "#img = np.squeeze(img)\n",
    "cv_img = np.transpose(img, (1, 2, 0))\n",
    "print('Image shape: ', cv_img.shape)\n",
    "plt.imshow(cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 227, 227)\n",
      "torch.Size([1, 3, 227, 227])\n",
      "[tensor([[401.2282]], device='cuda:0'), tensor([[401.2282]], device='cuda:0'), tensor([[636.4358]], device='cuda:0')]\n",
      "Prediction:  buoy\n",
      "Label:  buoy\n",
      "Image shape:  (227, 227, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff1c1f190b8>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = next(show_iter)\n",
    "img = data[0][0].numpy()\n",
    "print(img.shape)\n",
    "torch_img = np.expand_dims(img, axis=0)\n",
    "x = torch.tensor(torch_img).type('torch.FloatTensor').cuda()\n",
    "print(x.shape)\n",
    "scalar_data = [i.cuda() for i in data[2]]\n",
    "output = multiviewnet(x, scalar_data)\n",
    "print(scalar_data)\n",
    "pred_y = torch.max(output, 1)[1].cpu().data.numpy()\n",
    "print('Prediction: ', decoder[int(pred_y)])\n",
    "\n",
    "print('Label: ', decoder[data[1][0].numpy()])\n",
    "#img = img[:][:]\n",
    "#img = np.squeeze(img)\n",
    "cv_img = np.transpose(img, (1, 2, 0))\n",
    "print('Image shape: ', cv_img.shape)\n",
    "plt.imshow(cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type MultiViewNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(multiviewnet, '../model_scale/robotx_final.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
